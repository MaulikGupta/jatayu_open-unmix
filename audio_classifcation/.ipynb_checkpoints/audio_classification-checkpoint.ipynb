{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5e3117",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d1f8255e993a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwavfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "metadata = pd.read_csv('./meta.csv')\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c8277",
   "metadata": {},
   "source": [
    "<h1>#### Extracting MFCC's For every audio file</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bba46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e47a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b613c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-227.59056, 149.48346, -0.2679622, 6.281961, ...</td>\n",
       "      <td>crow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-217.32423, 147.93347, -2.3443086, 6.5044413,...</td>\n",
       "      <td>crow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-153.846, 124.30126, -29.816824, 8.058483, -3...</td>\n",
       "      <td>crow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-207.43484, 138.169, -14.45352, 18.09661, -7....</td>\n",
       "      <td>crow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-242.37227, 152.31898, 10.375188, 13.626253, ...</td>\n",
       "      <td>crow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature class\n",
       "0  [-227.59056, 149.48346, -0.2679622, 6.281961, ...  crow\n",
       "1  [-217.32423, 147.93347, -2.3443086, 6.5044413,...  crow\n",
       "2  [-153.846, 124.30126, -29.816824, 8.058483, -3...  crow\n",
       "3  [-207.43484, 138.169, -14.45352, 18.09661, -7....  crow\n",
       "4  [-242.37227, 152.31898, 10.375188, 13.626253, ...  crow"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac6c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1099807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b112712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ee3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding\n",
    "###y=np.array(pd.get_dummies(y))\n",
    "### Label Encoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c32e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f842280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ae1f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.64812469e+02,  6.22535706e+01, -7.71151495e+00, ...,\n",
       "         1.85784250e-01,  1.94310904e+00,  2.23951077e+00],\n",
       "       [-3.23632263e+02, -2.21011143e+01, -9.98628387e+01, ...,\n",
       "         3.06606388e+00, -2.34944868e+00, -2.17550397e+00],\n",
       "       [-3.12725372e+02,  2.83815689e+01, -9.98680191e+01, ...,\n",
       "         1.35116637e+00,  1.59060371e+00,  7.19787300e-01],\n",
       "       ...,\n",
       "       [-2.14212769e+02,  1.05224625e+02, -2.28969803e+01, ...,\n",
       "        -2.83538914e+00, -1.72062922e+00, -3.03701758e+00],\n",
       "       [-2.01290176e+02,  1.28786163e+02,  2.21617222e+00, ...,\n",
       "        -3.34015632e+00, -4.77180815e+00, -1.38094532e+00],\n",
       "       [-3.79684570e+02,  7.36921844e+01, -6.73128815e+01, ...,\n",
       "         2.80238152e+00,  2.78577876e+00,  1.82960975e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d47d24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9775bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 40)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed8aa560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 40)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1eaaedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8113d35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2267205",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99788fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e79f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d76125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67709c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ce5fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               4100      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 44,703\n",
      "Trainable params: 44,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12572822",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbc7f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 3s 74ms/step - loss: 35.4931 - accuracy: 0.3920 - val_loss: 4.7686 - val_accuracy: 0.6210\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.76858, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 22.7736 - accuracy: 0.3830 - val_loss: 1.5835 - val_accuracy: 0.5484\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.76858 to 1.58352, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 14.7430 - accuracy: 0.4134 - val_loss: 1.3698 - val_accuracy: 0.5484\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58352 to 1.36977, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 11.0606 - accuracy: 0.3745 - val_loss: 0.9062 - val_accuracy: 0.5081\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.36977 to 0.90622, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.1907 - accuracy: 0.3864 - val_loss: 0.9464 - val_accuracy: 0.5968\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.90622\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.4793 - accuracy: 0.4504 - val_loss: 1.0157 - val_accuracy: 0.5403\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.90622\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9861 - accuracy: 0.4288 - val_loss: 0.9607 - val_accuracy: 0.5968\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.90622\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5008 - accuracy: 0.3883 - val_loss: 0.9502 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.90622\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.1634 - accuracy: 0.4605 - val_loss: 0.8981 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.90622 to 0.89815, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.8675 - accuracy: 0.4778 - val_loss: 0.9272 - val_accuracy: 0.5806\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.89815\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2057 - accuracy: 0.4252 - val_loss: 0.9568 - val_accuracy: 0.6452\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.89815\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.4167 - accuracy: 0.4785 - val_loss: 0.9596 - val_accuracy: 0.6855\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.89815\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.3218 - accuracy: 0.4570 - val_loss: 0.9775 - val_accuracy: 0.6290\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.89815\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0017 - accuracy: 0.4820 - val_loss: 0.9784 - val_accuracy: 0.6290\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.89815\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.6975 - accuracy: 0.4707 - val_loss: 0.9905 - val_accuracy: 0.5968\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.89815\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8315 - accuracy: 0.5291 - val_loss: 1.0066 - val_accuracy: 0.5968\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.89815\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8073 - accuracy: 0.4451 - val_loss: 1.0161 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.89815\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8633 - accuracy: 0.4610 - val_loss: 1.0234 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.89815\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.6457 - accuracy: 0.4653 - val_loss: 1.0201 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.89815\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5168 - accuracy: 0.4318 - val_loss: 1.0096 - val_accuracy: 0.6048\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.89815\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.6197 - accuracy: 0.4488 - val_loss: 1.0008 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.89815\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.2974 - accuracy: 0.5143 - val_loss: 0.9919 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.89815\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.3059 - accuracy: 0.5241 - val_loss: 1.0000 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.89815\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2539 - accuracy: 0.4498 - val_loss: 1.0064 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.89815\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2485 - accuracy: 0.5069 - val_loss: 1.0077 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.89815\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2190 - accuracy: 0.4959 - val_loss: 1.0033 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.89815\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0862 - accuracy: 0.5442 - val_loss: 1.0006 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.89815\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0391 - accuracy: 0.5664 - val_loss: 0.9774 - val_accuracy: 0.5887\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.89815\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0272 - accuracy: 0.5272 - val_loss: 0.9574 - val_accuracy: 0.5968\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.89815\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0515 - accuracy: 0.5578 - val_loss: 0.9410 - val_accuracy: 0.6048\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.89815\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1551 - accuracy: 0.5548 - val_loss: 0.9308 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.89815\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0717 - accuracy: 0.5634 - val_loss: 0.9114 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.89815\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9520 - accuracy: 0.6215 - val_loss: 0.9006 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.89815\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0791 - accuracy: 0.5480 - val_loss: 0.8976 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.89815 to 0.89759, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0265 - accuracy: 0.5494 - val_loss: 0.8942 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.89759 to 0.89420, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0991 - accuracy: 0.5081 - val_loss: 0.8902 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.89420 to 0.89023, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1169 - accuracy: 0.4986 - val_loss: 0.8972 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.89023\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9955 - accuracy: 0.5801 - val_loss: 0.8896 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.89023 to 0.88961, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9823 - accuracy: 0.5601 - val_loss: 0.8830 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.88961 to 0.88298, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9653 - accuracy: 0.5434 - val_loss: 0.8730 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.88298 to 0.87299, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9658 - accuracy: 0.5335 - val_loss: 0.8781 - val_accuracy: 0.6129\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.87299\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9372 - accuracy: 0.5505 - val_loss: 0.8667 - val_accuracy: 0.6210\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.87299 to 0.86668, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9360 - accuracy: 0.5732 - val_loss: 0.8576 - val_accuracy: 0.6210\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.86668 to 0.85760, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8842 - accuracy: 0.5929 - val_loss: 0.8512 - val_accuracy: 0.6210\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.85760 to 0.85123, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8794 - accuracy: 0.5941 - val_loss: 0.8330 - val_accuracy: 0.6210\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.85123 to 0.83303, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8958 - accuracy: 0.5896 - val_loss: 0.8203 - val_accuracy: 0.6371\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.83303 to 0.82026, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9257 - accuracy: 0.5665 - val_loss: 0.8227 - val_accuracy: 0.6290\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.82026\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9540 - accuracy: 0.5786 - val_loss: 0.8190 - val_accuracy: 0.6371\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.82026 to 0.81903, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9435 - accuracy: 0.5874 - val_loss: 0.8032 - val_accuracy: 0.6371\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.81903 to 0.80319, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8834 - accuracy: 0.5944 - val_loss: 0.7892 - val_accuracy: 0.6371\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.80319 to 0.78922, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9028 - accuracy: 0.5710 - val_loss: 0.7863 - val_accuracy: 0.6371\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.78922 to 0.78625, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.8565 - accuracy: 0.5884 - val_loss: 0.7766 - val_accuracy: 0.6452\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.78625 to 0.77660, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7721 - accuracy: 0.6375 - val_loss: 0.7753 - val_accuracy: 0.6452\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.77660 to 0.77526, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8410 - accuracy: 0.5970 - val_loss: 0.7722 - val_accuracy: 0.6452\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.77526 to 0.77221, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7985 - accuracy: 0.6029 - val_loss: 0.7561 - val_accuracy: 0.6452\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.77221 to 0.75611, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8090 - accuracy: 0.6145 - val_loss: 0.7485 - val_accuracy: 0.6452\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.75611 to 0.74854, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7504 - accuracy: 0.6623 - val_loss: 0.7394 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.74854 to 0.73938, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7769 - accuracy: 0.6674 - val_loss: 0.7281 - val_accuracy: 0.6855\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.73938 to 0.72814, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7642 - accuracy: 0.6426 - val_loss: 0.7237 - val_accuracy: 0.6774\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.72814 to 0.72367, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8034 - accuracy: 0.6635 - val_loss: 0.7286 - val_accuracy: 0.6774\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.72367\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8030 - accuracy: 0.6341 - val_loss: 0.7102 - val_accuracy: 0.6855\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.72367 to 0.71021, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7535 - accuracy: 0.6457 - val_loss: 0.6982 - val_accuracy: 0.6855\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.71021 to 0.69819, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7876 - accuracy: 0.6357 - val_loss: 0.6879 - val_accuracy: 0.6935\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.69819 to 0.68792, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7778 - accuracy: 0.6019 - val_loss: 0.6948 - val_accuracy: 0.6855\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.68792\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7954 - accuracy: 0.6413 - val_loss: 0.6853 - val_accuracy: 0.7097\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.68792 to 0.68525, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7200 - accuracy: 0.6686 - val_loss: 0.6637 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.68525 to 0.66368, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6846 - accuracy: 0.6639 - val_loss: 0.6594 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.66368 to 0.65937, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7206 - accuracy: 0.6011 - val_loss: 0.6494 - val_accuracy: 0.7097\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.65937 to 0.64941, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7306 - accuracy: 0.6728 - val_loss: 0.6531 - val_accuracy: 0.7177\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.64941\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7399 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.7177\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.64941 to 0.64784, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7343 - accuracy: 0.6759 - val_loss: 0.6531 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.64784\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.6916 - val_loss: 0.6303 - val_accuracy: 0.7177\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.64784 to 0.63029, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.6673 - val_loss: 0.6198 - val_accuracy: 0.7177\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.63029 to 0.61982, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6803 - accuracy: 0.6766 - val_loss: 0.6010 - val_accuracy: 0.7177\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.61982 to 0.60097, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7107 - accuracy: 0.6576 - val_loss: 0.5895 - val_accuracy: 0.7177\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.60097 to 0.58952, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.6933 - val_loss: 0.5892 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.58952 to 0.58923, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6553 - accuracy: 0.7025 - val_loss: 0.5836 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.58923 to 0.58360, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6413 - accuracy: 0.6796 - val_loss: 0.5756 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.58360 to 0.57562, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6189 - accuracy: 0.6859 - val_loss: 0.5640 - val_accuracy: 0.7661\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.57562 to 0.56396, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5869 - accuracy: 0.7424 - val_loss: 0.5482 - val_accuracy: 0.7742\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.56396 to 0.54821, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6816 - accuracy: 0.6791 - val_loss: 0.5475 - val_accuracy: 0.7984\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.54821 to 0.54748, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6882 - accuracy: 0.7304 - val_loss: 0.5378 - val_accuracy: 0.7984\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.54748 to 0.53782, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6134 - accuracy: 0.7407 - val_loss: 0.5261 - val_accuracy: 0.8065\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.53782 to 0.52614, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6293 - accuracy: 0.6728 - val_loss: 0.5280 - val_accuracy: 0.7903\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.52614\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5501 - accuracy: 0.7495 - val_loss: 0.5045 - val_accuracy: 0.8226\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.52614 to 0.50452, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5729 - accuracy: 0.7455 - val_loss: 0.4986 - val_accuracy: 0.8226\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.50452 to 0.49863, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5913 - accuracy: 0.7526 - val_loss: 0.4751 - val_accuracy: 0.8548\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.49863 to 0.47511, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5387 - accuracy: 0.7650 - val_loss: 0.4713 - val_accuracy: 0.8548\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.47511 to 0.47128, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5256 - accuracy: 0.7759 - val_loss: 0.4605 - val_accuracy: 0.8306\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.47128 to 0.46047, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5606 - accuracy: 0.7509 - val_loss: 0.4590 - val_accuracy: 0.8548\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.46047 to 0.45895, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5667 - accuracy: 0.7521 - val_loss: 0.4403 - val_accuracy: 0.8629\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.45895 to 0.44029, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4945 - accuracy: 0.7902 - val_loss: 0.4288 - val_accuracy: 0.8468\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.44029 to 0.42881, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5064 - accuracy: 0.7592 - val_loss: 0.4229 - val_accuracy: 0.8548\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.42881 to 0.42286, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4631 - accuracy: 0.7915 - val_loss: 0.4219 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.42286 to 0.42190, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4700 - accuracy: 0.8234 - val_loss: 0.4025 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.42190 to 0.40254, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4968 - accuracy: 0.7946 - val_loss: 0.4077 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.40254\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.7952 - val_loss: 0.3942 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.40254 to 0.39424, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4456 - accuracy: 0.8245 - val_loss: 0.3703 - val_accuracy: 0.8710\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.39424 to 0.37034, saving model to saved_models/audio_classification.hdf5\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4388 - accuracy: 0.8110 - val_loss: 0.3786 - val_accuracy: 0.8871\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.37034\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4617 - accuracy: 0.7999 - val_loss: 0.3712 - val_accuracy: 0.8710\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.37034\n",
      "Training completed in time:  0:00:13.945220\n"
     ]
    }
   ],
   "source": [
    "# Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35262bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8709677457809448\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ee4d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05faebb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.72459106e+02,  1.02502556e+02, -4.11994438e+01,  4.55670452e+00,\n",
       "       -1.65558758e+01,  2.48143063e+01, -8.10355783e-01,  1.79815884e+01,\n",
       "       -7.88385582e+00,  1.79162884e+01, -4.71778584e+00,  1.36451921e+01,\n",
       "       -3.56350684e+00,  1.02704239e+01,  1.57364357e+00,  9.26171970e+00,\n",
       "       -3.06813791e-02,  8.73383999e+00,  3.96176791e+00,  6.96624327e+00,\n",
       "        1.93785226e+00,  6.31154442e+00,  2.83724236e+00,  3.94089293e+00,\n",
       "        3.68973994e+00,  5.17122507e+00,  2.06391811e+00,  3.26116681e+00,\n",
       "        1.60792029e+00,  5.27418947e+00,  2.13462853e+00,  2.71737528e+00,\n",
       "        4.05507135e+00,  2.00532413e+00,  3.22364306e+00,  4.26191378e+00,\n",
       "        2.80193353e+00,  3.44579190e-01,  3.78335667e+00,  1.30365098e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8e93921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maulik/anaconda3/envs/demo/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1,\n",
       "       0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0,\n",
       "       2, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9d615",
   "metadata": {},
   "source": [
    "<h1>model save</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b830df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('audio_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb94ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing Some Test Audio Data¶\n",
    "# Steps\n",
    "\n",
    "# Preprocess the new audio data\n",
    "# predict the classes\n",
    "# Invere transform your Predicted Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3afc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model('audio_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4430fb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blackbird'], dtype='<U9')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"/home/maulik/Desktop/Maulik_Gupta/Documents/jatayu_openunmix/open-unmix-pytorch/downloaded_audio/bbird_crowacc.wav\"\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "# print(mfccs_scaled_features)\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "# print(mfccs_scaled_features)\n",
    "# print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "# print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "prediction_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100b4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
